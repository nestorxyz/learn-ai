{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nestorxyz/coding/learn/ai/learn-ai/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "openai = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "DOMAIN = \"developer.mozilla.org\"\n",
    "\n",
    "def remove_newlines(series):\n",
    "  series = series.str.replace('\\n', ' ')\n",
    "  series = series.str.replace('\\\\n', ' ')\n",
    "  series = series.str.replace('  ', ' ')\n",
    "  series = series.str.replace('  ', ' ')\n",
    "  return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/pauldeveloper.mozilla.org/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m texts\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Get all the text files in the text directory\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/paul\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDOMAIN\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m   \u001b[38;5;66;03m# Open the file and read the text\u001b[39;00m\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m DOMAIN \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      9\u001b[0m     text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/pauldeveloper.mozilla.org/'"
     ]
    }
   ],
   "source": [
    "# Create a list to store the text files\n",
    "texts=[]\n",
    "\n",
    "# Get all the text files in the text directory\n",
    "for file in os.listdir(\"../data/paul/HowtoDoGreatWork.html\" + DOMAIN + \"/\"):\n",
    "\n",
    "  # Open the file and read the text\n",
    "  with open(\"text/\" + DOMAIN + \"/\" + file, \"r\", encoding=\"UTF-8\") as f:\n",
    "    text = f.read()\n",
    "    # we replace the last 4 characters to get rid of .txt, and replace _ with / to generate the URLs we scraped\n",
    "    filename = file[:-4].replace('_', '/')\n",
    "    \"\"\"\n",
    "    There are a lot of contributor.txt files that got included in the scrape, this weeds them out. There are also a lot of auth required urls that have been scraped to weed out as well\n",
    "    \"\"\" \n",
    "    if filename.endswith(\".txt\") or 'users/fxa/login' in filename:\n",
    "      continue\n",
    "\n",
    "    # then we replace underscores with / to get the actual links so we can cite contributions\n",
    "    texts.append(\n",
    "      (filename, text))\n",
    "\n",
    "# Create a dataframe from the list of texts\n",
    "df = pd.DataFrame(texts, columns=['fname', 'text'])\n",
    "\n",
    "# Set the text column to be the raw text with the newlines removed\n",
    "df['text'] = df.fname + \". \" + remove_newlines(df.text)\n",
    "df.to_csv('processed/scraped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cl100k_base tokenizer which is designed to work with the ada-002 model\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "df = pd.read_csv('processed/scraped.csv', index_col=0)\n",
    "df.columns = ['title', 'text']\n",
    "\n",
    "# Tokenize the text and save the number of tokens to a new column\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 1000  # Max number of tokens\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # This could be replaced with a token counting function if needed\n",
    "    length_function = len,  \n",
    "    chunk_size = chunk_size,\n",
    "    chunk_overlap  = 0,  # No overlap between chunks\n",
    "    add_start_index = False,  # We don't need start index in this case\n",
    ")\n",
    "\n",
    "shortened = []\n",
    "\n",
    "for row in df.iterrows():\n",
    "\n",
    "  # If the text is None, go to the next row\n",
    "  if row[1]['text'] is None:\n",
    "    continue\n",
    "\n",
    "  # If the number of tokens is greater than the max number of tokens, split the text into chunks\n",
    "  if row[1]['n_tokens'] > chunk_size:\n",
    "    # Split the text using LangChain's text splitter\n",
    "    chunks = text_splitter.create_documents([row[1]['text']])\n",
    "    # Append the content of each chunk to the 'shortened' list\n",
    "    for chunk in chunks:\n",
    "      shortened.append(chunk.page_content)\n",
    "\n",
    "  # Otherwise, add the text to the list of shortened texts\n",
    "  else:\n",
    "    shortened.append(row[1]['text'])\n",
    "\n",
    "df = pd.DataFrame(shortened, columns=['text'])\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embeddings'] = df.text.apply(lambda x: openai.embeddings.create(\n",
    "    input=x, model='text-embedding-ada-002').data[0].embedding)\n",
    "\n",
    "df.to_csv('processed/embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "from scipy import spatial\n",
    "import os\n",
    "\n",
    "def distances_from_embeddings(\n",
    "  query_embedding: List[float],\n",
    "  embeddings: List[List[float]],\n",
    "  distance_metric=\"cosine\",\n",
    ") -> List[List]:\n",
    "  \"\"\"Return the distances between a query embedding and a list of embeddings.\"\"\"\n",
    "  distance_metrics = {\n",
    "      \"cosine\": spatial.distance.cosine,\n",
    "      \"L1\": spatial.distance.cityblock,\n",
    "      \"L2\": spatial.distance.euclidean,\n",
    "      \"Linf\": spatial.distance.chebyshev,\n",
    "  }\n",
    "  distances = [\n",
    "      distance_metrics[distance_metric](query_embedding, embedding)\n",
    "      for embedding in embeddings\n",
    "  ]\n",
    "  return distances\n",
    "\n",
    "\n",
    "openai = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
    "\n",
    "df = pd.read_csv('processed/embeddings.csv', index_col=0)\n",
    "df['embeddings'] = df['embeddings'].apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context(question, df, max_len=1800):\n",
    "  \"\"\"\n",
    "    Create a context for a question by finding the most similar context from the dataframe\n",
    "    \"\"\"\n",
    "  # Get the embeddings for the question\n",
    "  q_embeddings = openai.embeddings.create(\n",
    "      input=question, model='text-embedding-ada-002').data[0].embedding\n",
    "\n",
    "  # Get the distances from the embeddings\n",
    "  df['distances'] = distances_from_embeddings(q_embeddings,\n",
    "                                              df['embeddings'].values,\n",
    "                                              distance_metric='cosine')\n",
    "\n",
    "  returns = []\n",
    "  cur_len = 0\n",
    "\n",
    "  # Sort by distance and add the text to the context until the context is too long\n",
    "  for i, row in df.sort_values('distances', ascending=True).iterrows():\n",
    "    # Add the length of the text to the current length\n",
    "    cur_len += row['n_tokens'] + 4\n",
    "\n",
    "    # If the context is too long, break\n",
    "    if cur_len > max_len:\n",
    "      break\n",
    "\n",
    "    # Else add it to the text that is being returned\n",
    "    returns.append(row[\"text\"])\n",
    "\n",
    "  # Return the context\n",
    "  return \"\\n\\n###\\n\\n\".join(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(df,\n",
    "                    model=\"gpt-3.5-turbo-1106\",\n",
    "                    question=\"What is the meaning of life?\",\n",
    "                    max_len=1800,\n",
    "                    debug=False,\n",
    "                    max_tokens=150,\n",
    "                    stop_sequence=None):\n",
    "  \"\"\"\n",
    "    Answer a question based on the most similar context from the dataframe texts\n",
    "    \"\"\"\n",
    "  context = create_context(\n",
    "      question,\n",
    "      df,\n",
    "      max_len=max_len,\n",
    "  )\n",
    "  # If debug, print the raw model response\n",
    "  if debug:\n",
    "    print(\"Context:\\n\" + context)\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "  try:\n",
    "    # Create a completions using the question and context\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\n",
    "            \"role\":\n",
    "            \"user\",\n",
    "            \"content\":\n",
    "            f\"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know.\\\" Try to site sources to the links in the context when possible.\\n\\nContext: {context}\\n\\n---\\n\\nQuestion: {question}\\nSource:\\nAnswer:\",\n",
    "        }],\n",
    "        temperature=0,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=1,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=stop_sequence,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "  except Exception as e:\n",
    "    print(e)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telegram bot main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You’ve successfully generated your own embeds, and created a way to ask questions about it. You could use this to generate summaries, or ask questions on just about any documents now. This is the process that a lot of companies that have “GPT Powered Docs” are doing under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from questions import answer_question\n",
    "\n",
    "df = pd.read_csv('processed/embeddings.csv', index_col=0)\n",
    "df['embeddings'] = df['embeddings'].apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def mozilla(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "      answer = answer_question(df, question=update.message.text, debug=True)\n",
    "      await context.bot.send_message(chat_id=update.effective_chat.id, text=answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Own implementation\n",
    "\n",
    "Aim: generate embeds for a given text and ask questions about it.\n",
    "\n",
    "## Embeds\n",
    "tokenize with tiktoken and save the number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
